{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "Change parallel arrays for checking accuracy to data frame instead <br>\n",
    "\n",
    "BEFORE PUTTING ON GITHUB!!! <br>\n",
    "Very small step in prediction mesh <br>\n",
    "Range of k-values to try <br>\n",
    "File name (not the partial data file) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import neighbors\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format data\n",
    "Indicate the name of the csv file with a column to each parameter, a row to each trial, and both row and column headers. The classification indicator must be the last column and contain either the integer 0 or 1 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataFileName = 'ParameterData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into data frame, ignoring the 1st column of trial indices\n",
    "with open(inputDataFileName) as f:\n",
    "    ncols = len(f.readline().split(','))\n",
    "\n",
    "parameterDF = pd.read_csv(inputDataFileName, delimiter=',', index_col=0, header=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the indexing column and reset the indices\n",
    "parameterDF = parameterDF.reset_index()\n",
    "del parameterDF['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "params = parameterDF.columns.tolist() # List of labels for indexing\n",
    "normDF = DataFrame()\n",
    "\n",
    "for param in params[:-1]: # Normalize each parameter column (not classification column)\n",
    "    normalizedData = ( parameterDF[param] - parameterDF[param].mean() ) / parameterDF[param].std()\n",
    "    normDF = pd.concat( [normDF, Series(normalizedData) ], axis=1 )\n",
    "    normDF.rename( columns={ 0:param } )\n",
    "\n",
    "# Add the classification column back in\n",
    "normDF = pd.concat( [normDF, parameterDF[ params[-1] ] ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by classification\n",
    "groups = normDF.groupby( normDF.columns[-1] )\n",
    "successGroup = groups.get_group(1)\n",
    "failureGroup = groups.get_group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal k and weight method for each pair of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numParams = len(params)-1 # For looping through parameters\n",
    "\n",
    "# Set the range of values of k around the typical standard for k=sqrt(n)\n",
    "# And only look at the odd values for k\n",
    "kMid = (int)( sqrt(parameterDF.shape[0]) ) # k nearest neighbors\n",
    "\n",
    "if kMid%2 is 0: # Want odd k\n",
    "    kMid = kMid + 1\n",
    "    \n",
    "kBeg = kMid - 50\n",
    "kEnd = kMid + 50\n",
    "ks = np.arange(kBeg, kEnd, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestClassifiersByWeight = []\n",
    "overallBestClassifiers = []\n",
    "overallBestKs = []\n",
    "overallBestAccValues = []\n",
    "overallBestWeights = []\n",
    "\n",
    "accScoresByWeight = []\n",
    "allAccScores = [] \n",
    "\n",
    "xMins = []\n",
    "xMaxs = []\n",
    "yMins = []\n",
    "yMaxs = []\n",
    "\n",
    "comparisons = []\n",
    "\n",
    "listOfWeights = ['uniform', 'distance']\n",
    "\n",
    "# Create list of comparison labels\n",
    "for xNum in range(numParams):\n",
    "    for yNum in range(numParams):\n",
    "        if xNum < yNum:\n",
    "            comparisons.append( params[xNum]+' vs '+params[yNum] )\n",
    "            \n",
    "# Fit and test the classifiers   \n",
    "for weight in listOfWeights:\n",
    "    \n",
    "    # Now inside the loop for each weight\n",
    "    print(str(weight)+':')\n",
    "    maxAccValues = []\n",
    "    bestKs =[]\n",
    "    accScoresByParams = []\n",
    "    bestClassifiers = []\n",
    "    \n",
    "    for rowNum in range(numParams):    \n",
    "    \n",
    "        for colNum in range(numParams): \n",
    "        \n",
    "            # Now inside the loop for each set of params\n",
    "        \n",
    "            if rowNum < colNum: # Exclude repeated plots \n",
    "            \n",
    "                # Now inside the loop for each set of params we use (without the ones we don't use)\n",
    "                print()\n",
    "                print( '     '+str(params[colNum]) + ' vs ' + str(params[rowNum]) + ':')                \n",
    "            \n",
    "                XTrains = []\n",
    "                XTests = []\n",
    "                yTrains = []\n",
    "                yTests = []\n",
    "\n",
    "                accuracyScores = [] \n",
    "                classifiers = []\n",
    "\n",
    "                # Get data\n",
    "                x1 = normDF[ params[colNum] ] # x-axis\n",
    "                x2 = normDF[ params[rowNum] ] # y-axis\n",
    "                X = np.column_stack( (x1, x2) )\n",
    "\n",
    "                # Get overall min and max values for plotting and prediction testing\n",
    "                xMin, xMax = x1.min(), x1.max() \n",
    "                yMin, yMax = x2.min(), x2.max()            \n",
    "                xMins.append(xMin), xMaxs.append(xMax), yMins.append(yMin), yMaxs.append(yMax)           \n",
    "\n",
    "                # Split data into training and testing samples\n",
    "                XTrain, XTest, yTrain, yTest = train_test_split(X, normDF[ params[-1] ])            \n",
    "                XTrains.append(XTrain), XTests.append(XTest), yTrains.append(yTrain), yTests.append(yTest)            \n",
    "                # USING SAME DATA FOR TRAINING VS TESTING FOR EACH K THAT WE TRY\n",
    "\n",
    "                # loop for each k\n",
    "                for k in ks:\n",
    "\n",
    "                    # Now inside the loop for each k for the set of params\n",
    "                    \n",
    "                    # Fit the classifier\n",
    "                    classifier = neighbors.KNeighborsClassifier( k, weights=weight ).fit(XTrain, yTrain)\n",
    "                    classifiers.append(classifier)                \n",
    "\n",
    "                    # Predict test data and find accuracy for this k\n",
    "                    predicted = classifier.predict( XTest )\n",
    "                    acc = accuracy_score( yTest, predicted )\n",
    "                    accuracyScores.append( acc )  \n",
    "                    print('          k = '+str(k)+'  accuracy = '+str(acc))\n",
    "\n",
    "                # Now back to the loop for each set of params that we use\n",
    "            \n",
    "                # Find the best accuracy for this set of params, get the cooresponding k and classifier\n",
    "                maxAccIndex, maxAccValue = max(enumerate(accuracyScores), key=operator.itemgetter(1))\n",
    "                bestK = ks[maxAccIndex]\n",
    "                maxAccValues.append( maxAccValue ) # actual accuracy\n",
    "                bestKs.append( bestK ) # just to know \n",
    "                bestClassifiers.append( classifiers[maxAccIndex] ) # for plotting  \n",
    "                accScoresByParams.append(accuracyScores)\n",
    "                \n",
    "                print()\n",
    "                print( '          Max accuracy at k=' + str(bestK) + ' at ' + str(maxAccValue) )\n",
    "                \n",
    "            # Back to the loop for each set of params (with repeats)\n",
    "        \n",
    "        \n",
    "    # Back to the loop for each weight \n",
    "    accScoresByWeight.append(accScoresByParams)\n",
    "    overallBestKs.append(bestKs)\n",
    "    bestClassifiersByWeight.append(bestClassifiers)\n",
    "    overallBestAccValues.append(maxAccValues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max(overallBestAccValues[0]) > max(overallBestAccValues[1]):\n",
    "    overallBestAccIndex, overallBestAccValue = max(enumerate(overallBestAccValues[0]), key=operator.itemgetter(1))\n",
    "    overallBestK = overallBestKs[0][overallBestAccIndex]\n",
    "    overallBestWeight = listOfWeights[0] \n",
    "    \n",
    "else:\n",
    "    overallBestAccIndex, overallBestAccValue = max(enumerate(overallBestAccValues[1]), key=operator.itemgetter(1))\n",
    "    overallBestK = overallBestKs[1][overallBestAccIndex]\n",
    "    overallBestWeight = listOfWeights[1]\n",
    "\n",
    "print('Maximum accuracy achieved with:')\n",
    "print('     '+str( np.hstack( (comparisons, comparisons))[overallBestAccIndex]))\n",
    "print('     accuracy = '+str(overallBestAccValue))\n",
    "print('     k = '+str(overallBestK))\n",
    "print('     weight = '+str(overallBestWeight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training data & prediction boundaries with optimized accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallBestClassifiers = []\n",
    "index = 0\n",
    "bestClassifiers = [ (a, b) for (a, b) in np.column_stack( (overallBestAccValues[0], overallBestAccValues[1]) ) ] \n",
    "for (a, b) in bestClassifiers:\n",
    "    if a > b:\n",
    "        overallBestClassifiers.append( bestClassifiersByWeight[0][index] )\n",
    "    else:\n",
    "        overallBestClassifiers.append( bestClassifiersByWeight[1][index] )\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.005  # Step size in the prediction mesh\n",
    "colMap = ListedColormap( ['#F08080', '#98FB98'] ) # Prediction mesh colors\n",
    "meshXs = []\n",
    "meshYs = []\n",
    "meshPredictions = [] \n",
    "\n",
    "# Predict a meshgrid of data points\n",
    "for num in range( len(overallBestClassifiers) ):\n",
    "    meshX, meshY = np.meshgrid(np.arange(xMins[num]-1, xMaxs[num]+1, h),\n",
    "                               np.arange(yMins[num]-1, yMaxs[num]+1, h))\n",
    "    meshXs.append(meshX), meshYs.append(meshY)\n",
    "    meshPredicted = overallBestClassifiers[num].predict(np.c_[meshX.ravel(), meshY.ravel()])\n",
    "    meshPredicted = meshPredicted.reshape(meshX.shape)\n",
    "    meshPredictions.append(meshPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the figure for plotting\n",
    "plt.rcParams.update({'figure.autolayout': False})\n",
    "plt.figure( figsize=(15,15) )\n",
    "G = gridspec.GridSpec(numParams, numParams)\n",
    "plt.subplots_adjust( hspace=0.1, wspace=0.1 ) \n",
    "plt.rc('font', size=20)\n",
    "plt.suptitle('Optimized KNN Classifiers', x=0.5, y=0.9)\n",
    "axarr = []\n",
    "indices = [] # For indexing row and column labels\n",
    "\n",
    "for xNum in range(numParams):\n",
    "    for yNum in range(numParams):\n",
    "        if xNum < yNum:\n",
    "            indices.append( (params[xNum],params[yNum]) ) \n",
    "            ax = plt.subplot( G[yNum-1, xNum] )\n",
    "            axarr.append(ax)\n",
    "            if xNum is 0:\n",
    "                plt.ylabel( params[yNum] )\n",
    "            if xNum is not 0:\n",
    "                plt.yticks( visible=False )\n",
    "            if yNum is numParams-1:\n",
    "                plt.xlabel( params[xNum] )\n",
    "                plt.xticks( rotation=45 )\n",
    "            if yNum is not numParams-1:\n",
    "                plt.xticks( visible=False ) \n",
    "                         \n",
    "# Plot \n",
    "for num in range(len(indices)):                \n",
    "    # Plot decision boundary\n",
    "    axarr[num].pcolormesh(meshXs[num], meshYs[num], meshPredictions[num], cmap=colMap)\n",
    "    \n",
    "    # Plot the training data \n",
    "    xSuccesses = successGroup[ indices[num][1] ].head(200)\n",
    "    ySuccesses = successGroup[ indices[num][0] ].head(200)\n",
    "    axarr[num].scatter( xSuccesses, ySuccesses, marker='o', color='#006400', alpha=0.7, label='success' )\n",
    "    \n",
    "    xFailures = failureGroup[ indices[num][1] ].head(200)\n",
    "    yFailures = failureGroup[ indices[num][0] ].head(200)\n",
    "    axarr[num].scatter( xFailures, yFailures, marker='s', color='#FF0000', alpha=0.7, label='failure')\n",
    "            \n",
    "    axarr[num].set_xlim( -3,3 )\n",
    "    axarr[num].set_ylim( -3,3 ) \n",
    "            \n",
    "#plt.legend(bbox_to_anchor=(1,1), loc=0, borderaxespad=-4)\n",
    "#plt.legend(bbox_to_anchor=(1,1), loc=1, borderaxespad=-4)\n",
    "plt.legend(bbox_to_anchor=(1,1), loc=2, borderaxespad=-11)\n",
    "plt.savefig('KNNPlotOptimized.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = list( range( len(comparisons) ) )\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots( figsize=(13, 8) )\n",
    "plt.rcParams.update({'figure.autolayout': False})\n",
    "plt.rc('font', size=16)\n",
    "plt.barh( pos, overallBestAccValues[0], width, color='#EEFF24', alpha=0.7, label=listOfWeights[0] )\n",
    "plt.barh([p+width for p in pos], overallBestAccValues[1], width, color='#F78F1E', label=listOfWeights[1])\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Accuracy vs Weight Method')\n",
    "ax.set_xticks(np.arange(0, 1.1, .1))\n",
    "ax.set_yticks([p+width for p in pos])\n",
    "ax.set_yticklabels(comparisons)\n",
    "plt.ylim( min(pos)-width, max(pos)+width*3)\n",
    "plt.xlim( 0, 1.0 )\n",
    "\n",
    "# Find pos to print the text at the end of each pair of bars by finding max of each pair of bars\n",
    "overallBestAccValuesReshaped = np.column_stack( (overallBestAccValues[0], overallBestAccValues[1]) )\n",
    "maxOverallBestAccValues = [ max(coord) for coord in overallBestAccValuesReshaped ]\n",
    "for yPos, xPos in enumerate(overallBestAccValues[0]):\n",
    "    ax.text(xPos+0.01, yPos+0.01, str(xPos))\n",
    "    \n",
    "for yPos, xPos in enumerate(overallBestAccValues[1]):\n",
    "    ax.text(xPos+0.01, yPos+0.5, str(xPos))\n",
    "    \n",
    "plt.grid()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = plt.legend(handles, labels, bbox_to_anchor=(1.05, 0.55), loc=2, borderaxespad=0.)\n",
    "plt.savefig('AccuracyVsWeight_plot1.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = list( range( len(comparisons) ) )\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots( figsize=(13, 8) )\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "plt.rc('font', size=16)\n",
    "plt.barh( pos, overallBestAccValues[0], width, color='#EEFF24', alpha=0.7, label=listOfWeights[0] )\n",
    "plt.barh([p+width for p in pos], overallBestAccValues[1], width, color='#F78F1E', label=listOfWeights[1])\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_xticks(np.arange(0, 1.1, .1))\n",
    "ax.set_yticks([p+width for p in pos])\n",
    "ax.set_yticklabels(comparisons)\n",
    "plt.ylim( min(pos)-width, max(pos)+width*3)\n",
    "plt.xlim( 0, 1.0 )\n",
    "\n",
    "# Find pos to print the text at the end of each pair of bars by finding max of each pair of bars\n",
    "overallBestAccValuesReshaped = np.column_stack( (overallBestAccValues[0], overallBestAccValues[1]) )\n",
    "maxOverallBestAccValues = [ max(coord) for coord in overallBestAccValuesReshaped ]\n",
    "\n",
    "for yPos, xPos in enumerate(maxOverallBestAccValues):\n",
    "    ax.text(xPos+0.01, yPos+0.25, str(xPos))    \n",
    "    \n",
    "plt.grid()\n",
    "diffInBestAccValues = [ (a-b) for (a,b) in overallBestAccValuesReshaped ]\n",
    "ax.set_title('Accuracy vs Weight Method (max diff='+str(max(diffInBestAccValues))+')')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = plt.legend(handles, labels, bbox_to_anchor=(1.05, 0.55), loc=2, borderaxespad=0.)\n",
    "plt.savefig('AccuracyVsWeight_plot2.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the accuracy (horizontal bar chart)\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "yAxis = np.arange(len(comparisons))\n",
    "plt.rc('font', size=18)\n",
    "plt.figure( figsize=(13,8) )\n",
    "plt.barh(yAxis, overallBestAccValues[0], align='center')\n",
    "\n",
    "for yPos, xPos in enumerate(overallBestAccValues[0]):\n",
    "    plt.gca().text(xPos+0.01, yPos-0.2, str(xPos))\n",
    "    \n",
    "plt.yticks(yAxis, comparisons)\n",
    "plt.grid()\n",
    "plt.xlim(0, 1.0)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Uniform Weight Method - Best Accuracy Achieved')\n",
    "plt.savefig('UniformAccuracy.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the accuracy (horizontal bar chart)\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "yAxis = np.arange(len(comparisons))\n",
    "plt.rc('font', size=18)\n",
    "plt.figure( figsize=(13,8) )\n",
    "plt.barh(yAxis, overallBestAccValues[1], align='center')\n",
    "\n",
    "for yPos, xPos in enumerate(overallBestAccValues[1]):\n",
    "    plt.gca().text(xPos+0.01, yPos-0.2, str(xPos))\n",
    "\n",
    "plt.yticks(yAxis, comparisons)\n",
    "plt.grid()\n",
    "plt.xlim(0, 1.0)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.title('Distance Weight Method - Best Accuracy Achieved')\n",
    "plt.savefig('DistanceAccuracy.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisonsWOspaces = comparisons\n",
    "for comparison in comparisonsWOspaces:\n",
    "    comparison=comparison.replace(' ', '')\n",
    "    comparison=comparison.replace('vs', 'Vs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range( len(comparisons) ):\n",
    "\n",
    "    plt.figure( figsize=(10, 5) )\n",
    "    plt.title(comparisons[index])\n",
    "    plt.plot( ks, accScoresByWeight[0][index], linewidth=3, label=listOfWeights[0] )\n",
    "    plt.plot( ks, accScoresByWeight[1][index], linewidth=3, label=listOfWeights[1] )\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlim( min(ks), max(ks) )\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    lgd = plt.legend(handles, labels, bbox_to_anchor=(1.05, .55), loc=2, borderaxespad=0.)\n",
    "    plt.savefig('KvsAccuracy_'+comparisonsWOspaces[index]+'.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(8, 7) )\n",
    "plt.title('K vs Accuracy ('+listOfWeights[0]+' weight)')\n",
    "\n",
    "for index in range( len(comparisons) ):\n",
    "    plt.plot( ks, accScoresByWeight[0][index], linewidth=3, label=comparisons[index] )\n",
    "    \n",
    "plt.xlim( min(ks), max(ks) )\n",
    "plt.ylim( min( min(accScoresByWeight[0]) )-0.01, max( max(accScoresByWeight[0]) )+0.01 )\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "lgd = plt.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.savefig('KvsAccuracy_'+listOfWeights[0]+'.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(8, 7) )\n",
    "plt.title('K vs Accuracy ('+listOfWeights[1]+' weight)')\n",
    "\n",
    "for index in range( len(comparisons) ):\n",
    "    plt.plot( ks, accScoresByWeight[1][index], linewidth=3, label=comparisons[index] )\n",
    "    \n",
    "plt.xlim( min(ks), max(ks) )\n",
    "plt.ylim( min( min(accScoresByWeight[1]) )-0.01, max( max(accScoresByWeight[1]) )+0.01 )\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "lgd = plt.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.savefig('KvsAccuracy_'+listOfWeights[1]+'.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the figure for plotting\n",
    "plt.rcParams.update({'figure.autolayout': False})\n",
    "plt.figure( figsize=(15,15) )\n",
    "plt.suptitle('K vs Accuracy', x=0.5, y=0.9)\n",
    "G = gridspec.GridSpec(numParams, numParams)\n",
    "plt.subplots_adjust( hspace=0.2, wspace=0.8 ) \n",
    "plt.rc('font', size=18)\n",
    "axarr = []\n",
    "indices = [] # For indexing row and column labels\n",
    "\n",
    "for xNum in range(numParams):\n",
    "       \n",
    "    for yNum in range(numParams):\n",
    "        \n",
    "        if xNum < yNum:            \n",
    "            indices.append( (params[xNum],params[yNum]) ) \n",
    "            ax = plt.subplot( G[yNum-1, xNum] )\n",
    "            axarr.append(ax)\n",
    "            \n",
    "            if xNum is yNum-1:\n",
    "                ax.yaxis.set_label_coords(1.8, 0.5)\n",
    "                plt.ylabel( params[yNum], rotation=0 )\n",
    "                \n",
    "            #if xNum is not 0:\n",
    "            #    plt.yticks( visible=False )\n",
    "                \n",
    "            if yNum is numParams-1:\n",
    "                plt.xlabel( params[xNum] )\n",
    "                plt.xticks( rotation=45 )\n",
    "                \n",
    "            if yNum is not numParams-1:\n",
    "                plt.xticks( visible=False )        \n",
    "               \n",
    "\n",
    "# Plot \n",
    "for index in range( len(indices) ):    \n",
    "    \n",
    "    axarr[index].plot( ks, accScoresByWeight[0][index], color='#006400', linewidth=3, alpha=0.7, label=listOfWeights[0] )\n",
    "    axarr[index].plot( ks, accScoresByWeight[1][index], color='#FF0000', linewidth=3, alpha=0.7, label=listOfWeights[1] )\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "lgd = plt.legend(handles, labels, bbox_to_anchor=(1.7,1.7), loc=2, borderaxespad=-8)\n",
    "plt.savefig('KvsAccuracy_gridspec.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the figure for plotting\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "size = int(len(comparisons)/4)+1\n",
    "f, axarr = plt.subplots( 4, size, figsize=(20,20), sharex=True, sharey=False)\n",
    "plt.subplots_adjust( hspace=1 ) \n",
    "plt.rc('font', size=20)\n",
    "plt.suptitle('K vs Accuracy', x=0.5, y=1.02, size=26)\n",
    "             \n",
    "# Plot\n",
    "for index in range( len(comparisons) ):  \n",
    "\n",
    "    if index < size:\n",
    "        row = 0\n",
    "    elif index < size*2:\n",
    "        row = 1\n",
    "    elif index < size*3:\n",
    "        row = 2\n",
    "    else:\n",
    "        row = 3\n",
    "        \n",
    "    axarr[row][index-row*size].plot( ks, accScoresByWeight[0][index], linewidth=3, color='#EEFF24', alpha=0.7 )\n",
    "    axarr[row][index-row*size].plot( ks, accScoresByWeight[1][index], linewidth=3, color='#F78F1E' )\n",
    "    axarr[row][index-row*size].set_xlabel( 'K' )\n",
    "    axarr[row][index-row*size].set_ylabel( 'Accuracy' )\n",
    "    axarr[row][index-row*size].set_xlim( min(ks), max(ks) )\n",
    "    axarr[row][index-row*size].set_title( str(comparisons[index]) )\n",
    "    \n",
    "    for tick in axarr[row][index-row*size].get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "\n",
    "plt.xticks( rotation=45 )\n",
    "plt.savefig('KvsAccuracy_subplots.png')                \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
